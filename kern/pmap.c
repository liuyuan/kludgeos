/* See COPYRIGHT for copyright information. */

#include <inc/x86.h>
#include <inc/mmu.h>
#include <inc/error.h>
#include <inc/string.h>
#include <inc/assert.h>
#include <inc/spinlock.h>
#include <inc/atomic.h>
#include <inc/trap.h>

#include <kern/pmap.h>
#include <kern/kclock.h>
#include <kern/env.h>
#include <kern/mp.h>
#include <kern/dev/bga.h>

// These variables are set by i386_detect_memory()
static physaddr_t maxpa;	// Maximum physical address
size_t npage;			// Amount of physical memory (in pages)
static size_t basemem;		// Amount of base memory (in bytes)
static size_t extmem;		// Amount of extended memory (in bytes)

// These variables are set in i386_vm_init()
pde_t* boot_pgdir;		// Virtual address of boot time page directory
physaddr_t boot_cr3;		// Physical address of boot time page directory
static char* boot_freemem;	// Pointer to next byte of free mem

struct Page* pages;		// Virtual address of physical page array
static struct Page_list page_free_list;	// Free list of physical pages

static struct Spinlock page_table_lock;
struct Spinlock tlb_lock;
uintptr_t *tlb_va;   /* Contains the address of TLB to be shooted down */
volatile uint32_t booted;			/* Indicated if all the CPU(s) get initialized */
// Global descriptor table.
//
// The kernel and user segments are identical (except for the DPL).
// To load the SS register, the CPL must equal the DPL.  Thus,
// we must duplicate the segments for the user and the kernel.
//
struct Segdesc gdt[] =
{
	// 0x0 - unused (always faults -- for trapping NULL far pointers)
	SEG_NULL,

	// 0x8 - kernel code segment
	[GD_KT >> 3] = SEG(STA_X | STA_R, 0x0, 0xffffffff, 0),

	// 0x10 - kernel data segment
	[GD_KD >> 3] = SEG(STA_W, 0x0, 0xffffffff, 0),

	// 0x18 - user code segment
	[GD_UT >> 3] = SEG(STA_X | STA_R, 0x0, 0xffffffff, 3),

	// 0x20 - user data segment
	[GD_UD >> 3] = SEG(STA_W, 0x0, 0xffffffff, 3),

	// 0x28 - tss, initialized in idt_init()
	[GD_TSS >> 3] = SEG_NULL
};

struct Pseudodesc gdt_pd = {
	sizeof(gdt) - 1, (unsigned long) gdt
};

static int
nvram_read(int r)
{
	return mc146818_read(r) | (mc146818_read(r + 1) << 8);
}

void
i386_detect_memory(void)
{
	// CMOS tells us how many kilobytes there are
	basemem = ROUNDDOWN(nvram_read(NVRAM_BASELO)*1024, PGSIZE);
	extmem = ROUNDDOWN(nvram_read(NVRAM_EXTLO)*1024, PGSIZE);

	// Calculate the maximum physical address based on whether
	// or not there is any extended memory.  See comment in <inc/mmu.h>.
	if (extmem)
		maxpa = EXTPHYSMEM + extmem;
	else
		maxpa = basemem;

	npage = maxpa / PGSIZE;

	cprintf("Physical memory: %dK available, ", (int)(maxpa/1024));
	cprintf("base = %dK, extended = %dK\n", (int)(basemem/1024), (int)(extmem/1024));
}

// --------------------------------------------------------------
// Set up initial memory mappings and turn on MMU.
// --------------------------------------------------------------

static void check_boot_pgdir(void);
static void check_page_alloc();
static void page_check(void);
static void boot_map_segment(pde_t *pgdir, uintptr_t la, size_t size, physaddr_t pa, int perm);

//
// A simple physical memory allocator, used only a few times
// in the process of setting up the virtual memory system.
// page_alloc() is the real allocator.
//
// Allocate n bytes of physical memory aligned on an 
// align-byte boundary.  Align must be a power of two.
// Return kernel virtual address.  Returned memory is uninitialized.
//
// If we're out of memory, boot_alloc should panic.
// This function may ONLY be used during initialization,
// before the page_free_list has been set up.
// 
static void*
boot_alloc(uint32_t n, uint32_t align)
{
	extern char end[];
	void *v;

	// Initialize boot_freemem if this is the first time.
	// 'end' is a magic symbol automatically generated by the linker,
	// which points to the end of the kernel's bss segment -
	// i.e., the first virtual address that the linker
	// did _not_ assign to any kernel code or global variables.
	if (boot_freemem == 0)
		boot_freemem = end;

	boot_freemem = ROUNDUP(boot_freemem, align);
	v = boot_freemem;
	boot_freemem += n;
	if (PADDR(boot_freemem) > maxpa)
		panic("boot_alloc: Out of memory!\n");
	return v;
}

// Set up a two-level page table:
//    boot_pgdir is its linear (virtual) address of the root
//    boot_cr3 is the physical adresss of the root
// Then turn on paging.  Then effectively turn off segmentation.
// (i.e., the segment base addrs are set to zero).
// 
// This function only sets up the kernel part of the address space
// (ie. addresses >= UTOP).  The user part of the address space
// will be setup later.
//
// From UTOP to ULIM, the user is allowed to read but not write.
// Above ULIM the user cannot read (or write). 
void
i386_vm_init(void)
{
	pde_t* pgdir;
	uint32_t cr0;
	size_t n;

	// create initial page directory.
	pgdir = boot_alloc(PGSIZE, PGSIZE);
	memset(pgdir, 0, PGSIZE);
	boot_pgdir = pgdir;
	boot_cr3 = PADDR(pgdir);


	// Recursively insert PD in itself as a page table, to form
	// a virtual page table at virtual address VPT.
	// (For now, you don't have understand the greater purpose of the
	// following two lines.)
	pgdir[PDX(VPT)] = boot_cr3 | PTE_W | PTE_P;

	pgdir[PDX(UVPT)] = boot_cr3 | PTE_U | PTE_P;

	// Make 'pages' point to an array of size 'npage' of 'struct Page'.
	// The kernel uses this structure to keep track of physical pages;
	// 'npage' equals the number of physical pages in memory.  User-level
	// programs will get read-only access to the array as well.
	pages = (struct Page *)boot_alloc(sizeof(struct Page) * npage, PGSIZE);

	envs = (struct Env *)boot_alloc(sizeof(struct Env) * NENV, PGSIZE);
	

	page_init();


	// Now we set up virtual memory 
	boot_map_segment(boot_pgdir, UPAGES, ROUNDUP(sizeof(struct Page) * npage, PGSIZE), 
			 PADDR(pages), PTE_U | PTE_P);

	boot_map_segment(boot_pgdir, UENVS, ROUNDUP(sizeof(struct Env) * NENV, PGSIZE),
			 PADDR(envs), PTE_U | PTE_P);

	boot_map_segment(boot_pgdir, KSTACKTOP-KSTKSIZE, KSTKSIZE, PADDR(bootstack), PTE_W | PTE_P);

	boot_map_segment(boot_pgdir, KERNBASE, (2^32) - KERNBASE, 0, PTE_W | PTE_P);

	// Current mapping: VA KERNBASE+x => PA x.
	//     (segmentation base=-KERNBASE and paging is off)

	// From here on down we must maintain this VA KERNBASE + x => PA x
	// mapping, even though we are turning on paging and reconfiguring
	// segmentation.

	// Map VA 0:4MB same as VA KERNBASE, i.e. to PA 0:4MB.
	// (Limits our kernel to <4MB)
	pgdir[0] = pgdir[PDX(KERNBASE)];

	// Install page table.
	lcr3(boot_cr3);

	// Turn on paging.
	cr0 = rcr0();
	cr0 |= CR0_PE|CR0_PG|CR0_AM|CR0_WP|CR0_NE|CR0_TS|CR0_EM|CR0_MP;
	cr0 &= ~(CR0_TS|CR0_EM);
	lcr0(cr0);

	// Current mapping: KERNBASE+x => x => x.
	// (x < 4MB so uses paging pgdir[0])

	// Reload all segment registers.
	asm volatile("lgdt gdt_pd");
	asm volatile("movw %%ax,%%gs" :: "a" (GD_UD|3));
	asm volatile("movw %%ax,%%fs" :: "a" (GD_UD|3));
	asm volatile("movw %%ax,%%es" :: "a" (GD_KD));
	asm volatile("movw %%ax,%%ds" :: "a" (GD_KD));
	asm volatile("movw %%ax,%%ss" :: "a" (GD_KD));
	asm volatile("ljmp %0,$1f\n 1:\n" :: "i" (GD_KT));  // reload cs
	asm volatile("lldt %%ax" :: "a" (0));

	// Final mapping: KERNBASE+x => KERNBASE+x => x.

	// This mapping was only used after paging was turned on but
	// before the segment registers were reloaded.
	pgdir[0] = 0;

	// Flush the TLB for good measure, to kill the pgdir[0] mapping.
	lcr3(boot_cr3);

	// Remap our screen
	bga_init2();
}
		
// --------------------------------------------------------------
// Tracking of physical pages.
// The 'pages' array has one 'struct Page' entry per physical page.
// Pages are reference counted, and free pages are kept on a linked list.
// --------------------------------------------------------------
void
page_init(void)
{
	int i;
	LIST_INIT(&page_free_list);
	/* page 0 in use */
	for (i = 1; i < IOPHYSMEM / PGSIZE; i++) {
		atomic_set(&pages[i].pp_ref, 0);
		//spin_init(&pages[i].pp_lock);
		LIST_INSERT_HEAD(&page_free_list, &pages[i], pp_link);
	}
	/* pages of IO hole and kernel in use */
	for (i = PADDR(boot_freemem) / PGSIZE; i < npage; i++) {
		atomic_set(&pages[i].pp_ref, 0);
		//spin_init(&pages[i].pp_lock);
		LIST_INSERT_HEAD(&page_free_list, &pages[i], pp_link);
	}
	spin_init(&tlb_lock);
	spin_init(&page_table_lock);
}


// Initialize a Page structure.
// The result has null links and 0 refcount.
// Note that the corresponding physical page is NOT initialized!
static void
page_initpp(struct Page *pp)
{
	memset(pp, 0, sizeof(*pp));
}


// Allocates a physical page.
// *pp_store -- is set to point to the Page struct of the newly allocated
// page
//
// RETURNS 
//   0 -- on success
//   -E_NO_MEM -- otherwise
int
page_alloc(struct Page **pp_store)
{
	spin_lock(&page_table_lock);
	if (!LIST_EMPTY(&page_free_list)) {
		*pp_store = LIST_FIRST(&page_free_list);
		LIST_REMOVE(*pp_store, pp_link);
		spin_unlock(&page_table_lock);
		return 0;
	}
	spin_unlock(&page_table_lock);
	return -E_NO_MEM;
}


// Return a page to the free list.
// (This function should only be called when pp->pp_ref reaches 0.)
void
page_free(struct Page *pp)
{
	// Fill this function in
	spin_lock(&page_table_lock);
	assert(atomic_read(&pp->pp_ref) == 0);
	LIST_INSERT_HEAD(&page_free_list, pp, pp_link);
	spin_unlock(&page_table_lock);
}


// Decrement the reference count on a page,
// freeing it if there are no more refs.
void
page_decref(struct Page* pp)
{
	assert(atomic_read(&pp->pp_ref) > 0);
	if (atomic_dec_and_test(&pp->pp_ref))
		page_free(pp);
}

// Given 'pgdir', a pointer to a page directory, pgdir_walk returns
// a pointer to the page table entry (PTE) for linear address 'va'.
pte_t *
pgdir_walk(pde_t *pgdir, const void *va, int create)
{
	// Fill this function in
	struct Page *pg;
	pte_t *p;
	
	if (!(pgdir[PDX(va)] & PTE_P)) {
		if (create == 0)
			return NULL;
		else if (page_alloc(&pg) == -E_NO_MEM)
			return NULL;
		else {						
		/* Now we got a page frame for page table */
			memset(page2kva(pg), 0, PGSIZE);
			if (booted && ismp) { /* TLB shootdown */
				spin_lock(&tlb_lock);
				mp_ipi_broadcast(0, T_TLBFLUSH);
				pgdir[PDX(va)] = page2pa(pg) | PTE_U | PTE_W | PTE_P;
				tlb_va = (uintptr_t *)va;
				tlb_invalidate(curenv->env_pgdir, (void *)va);
				spin_unlock(&tlb_lock);
			} else
				pgdir[PDX(va)] = page2pa(pg) | PTE_U | PTE_W | PTE_P;
			atomic_inc(&pg->pp_ref);
		}
		/* found */
	}
	p = (pte_t *)KADDR(PTE_ADDR(pgdir[PDX(va)]));
	return &p[PTX(va)];
}


// Map the physical page 'pp' at virtual address 'va'.
// RETURNS: 
//   0 on success
//   -E_NO_MEM, if page table couldn't be allocated
int
page_insert(pde_t *pgdir, struct Page *pp, void *va, int perm) 
{
	// Fill this function in
	pte_t *p;
	if (!(p = pgdir_walk(pgdir, va, 1)))
		return -E_NO_MEM;

	atomic_inc(&pp->pp_ref);	/* Succeed */
	if (*p & PTE_P)
		page_remove(pgdir, va);
	if (booted && ismp) {
		assert(!(*p & PTE_P));
		spin_lock(&tlb_lock);
		mp_ipi_broadcast(0, T_TLBFLUSH);
		*p = page2pa(pp) | perm | PTE_P;
		tlb_va = (uintptr_t *)va;
		tlb_invalidate(curenv->env_pgdir, va);
		spin_unlock(&tlb_lock);
	} else
		*p = page2pa(pp) | perm | PTE_P;
	return 0;
}


// Map [la, la+size) of linear address space to physical [pa, pa+size)
// in the page table rooted at pgdir.  Size is a multiple of PGSIZE.
// Use permission bits perm|PTE_P for the entries.
static void
boot_map_segment(pde_t *pgdir, uintptr_t la, size_t size, physaddr_t pa, int perm)
{
	pte_t *p;
	int i;
	for (i = 0; i < size / PGSIZE; i++, la += PGSIZE) {
		p = pgdir_walk(pgdir, (void*)la, 1);
		*p = (pa + PGSIZE * i) | perm | PTE_P;
	}
}


// Return the page mapped at virtual address 'va'.
// Return 0 if there is no page mapped at va.
struct Page *
page_lookup(pde_t *pgdir, void *va, pte_t **pte_store)
{
	pte_t *p;
	if ((p = pgdir_walk(pgdir, va, 0)) == 0)
		return 0;	
	if (!(*p & PTE_P))
		return 0;	/* No page mapped */
	if (pte_store != NULL)
		*pte_store = p;

	return pa2page(*p);
}

// Unmaps the physical page at virtual address 'va'.
void
page_remove(pde_t *pgdir, void *va)
{
	// Fill this function in
	struct Page *pp;
	pte_t *pte_store;
	pp = page_lookup(pgdir, va, &pte_store);
	if (pp != 0) {
		page_decref(pp);
		*pte_store = 0;
		if (ismp) {
			spin_lock(&tlb_lock);
			mp_ipi_broadcast(0, T_TLBFLUSH);
			tlb_invalidate(pgdir, va);
			tlb_va = (uintptr_t *)va;
			tlb_invalidate(curenv->env_pgdir, va);
			spin_unlock(&tlb_lock);
		} else
			tlb_invalidate(pgdir, va);
	}
}

//
// Invalidate a TLB entry, but only if the page tables being
// edited are the ones currently in use by the processor.
//
void
tlb_invalidate(pde_t *pgdir, void *va)
{
	// Flush the entry only if we're modifying the current address space.
	if (!curenv || curenv->env_pgdir == pgdir)
		invlpg(va);
}

static uintptr_t user_mem_check_addr;

//
// Check that an environment is allowed to access the range of memory
// [va, va+len) with permissions 'perm | PTE_P'.
// Normally 'perm' will contain PTE_U at least, but this is not required.
// 'va' and 'len' need not be page-aligned; you must test every page that
// contains any of that range.  You will test either 'len/PGSIZE',
// 'len/PGSIZE + 1', or 'len/PGSIZE + 2' pages.
//
// A user program can access a virtual address if (1) the address is below
// ULIM, and (2) the page table gives it permission.  These are exactly
// the tests you should implement here.
//
// If there is an error, set the 'user_mem_check_addr' variable to the first
// erroneous virtual address.
//
// Returns 0 if the user program can access this range of addresses,
// and -E_FAULT otherwise.
//
int
user_mem_check(struct Env *env, const void *va, size_t len, int perm)
{
	// LAB 3: Your code here. 
	uintptr_t b, e;
	int i;
	pte_t *p;

	b = (uintptr_t)ROUNDDOWN(va, PGSIZE);
	e = (uintptr_t)ROUNDUP(va + len, PGSIZE);
	for (i = 0; i < (b - e) / PGSIZE; i++, b += PGSIZE) {
		if ((uintptr_t)va >= ULIM)
			goto fault;
			
		if (!(p = pgdir_walk(env->env_pgdir, va, 0)))
			goto fault;
		/* Already PTE_P */
		if (!((*p & perm) == perm))
			goto fault;
	}
	return 0;
fault:
	user_mem_check_addr = b > (uintptr_t)va ? b : (uintptr_t)va;
	return -E_FAULT;
}

//
// Checks that environment 'env' is allowed to access the range
// of memory [va, va+len) with permissions 'perm | PTE_U'.
// If it can, then the function simply returns.
// If it cannot, 'env' is destroyed.
//
void
user_mem_assert(struct Env *env, const void *va, size_t len, int perm)
{
	if (user_mem_check(env, va, len, perm | PTE_U) < 0) {
		cprintf("[%08x] user_mem_check assertion failure for "
			"va %08x\n", curenv->env_id, user_mem_check_addr);
		env_destroy(env);	// may not return
	}
}
